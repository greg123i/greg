import torch
import os
import sys
import copy
import random

# Add project root to path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '..'))
if project_root not in sys.path:
    sys.path.append(project_root)

from lib.model_interface import ModelInterface
from lib.brain.config import BrainConfig
from misc.data_classifier import get_ollama_rating

class DPOTrainer:
    def __init__(self):
        self.interface = ModelInterface()
        # Load latest checkpoint
        self.interface.load_checkpoint()
        
        # Reference Model (Frozen copy of initial/current model)
        self.ref_brain = copy.deepcopy(self.interface.brain)
        self.ref_brain.eval()
        for param in self.ref_brain.parameters():
            param.requires_grad = False
            
        self.beta = 0.1
        self.data_dir = os.path.join(project_root, 'app', 'data')
        self.generated_dir = os.path.join(self.data_dir, 'generated_dpo')
        os.makedirs(self.generated_dir, exist_ok=True)

    def generate_and_rate(self, num_samples=10):
        """
        Generates samples, rates them, and forms pairs.
        """
        print(f"Generating {num_samples} samples...")
        samples = []
        
        # 1. Generate
        for i in range(num_samples):
            # We need a prompt. Let's pick a random start from 'good' data if available, or generic.
            prompt = "Once upon a time"
            
            # Use interface to generate (we might need to expose a generate method or use internal)
            # For now, using a simplified generation call or mocking it
            # self.interface.brain...
            # To keep it simple for this script, let's assume we use the interface's prediction logic
            # tailored for generation.
            
            # ... Generation Logic ...
            generated_text = "Sample text generated by Greg AI..." # Placeholder
            
            # 2. Rate
            rating = get_ollama_rating(generated_text)
            samples.append({'text': generated_text, 'rating': rating})
            print(f"Sample {i}: {rating}")
            
        return samples

    def form_pairs(self, samples):
        """
        Forms (chosen, rejected) pairs from rated samples.
        """
        chosen = []
        rejected = []
        
        for s in samples:
            r = s['rating']
            if r in ['good', 'really_good']:
                chosen.append(s['text'])
            elif r in ['bad', 'really_bad']:
                rejected.append(s['text'])
                
        # Form pairs
        pairs = []
        # Simple pairing: match every chosen with a random rejected
        import itertools
        if chosen and rejected:
            for c in chosen:
                r = random.choice(rejected)
                pairs.append((c, r))
                
        print(f"Formed {len(pairs)} DPO pairs.")
        return pairs

    def compute_log_prob(self, model, text):
        """
        Computes log probability of text under model.
        """
        # This requires running the model forward and summing log_softmax of logits at target indices.
        # Placeholder logic
        return torch.tensor(0.0, requires_grad=True)

    def dpo_step(self, pairs):
        """
        Performs one DPO optimization step.
        """
        optimizer = torch.optim.Adam(self.interface.brain.parameters(), lr=1e-5)
        
        total_loss = 0
        for chosen_text, rejected_text in pairs:
            optimizer.zero_grad()
            
            # Calculate log probs
            # policy_chosen_logps = self.compute_log_prob(self.interface.brain, chosen_text)
            # policy_rejected_logps = self.compute_log_prob(self.interface.brain, rejected_text)
            # ref_chosen_logps = self.compute_log_prob(self.ref_brain, chosen_text)
            # ref_rejected_logps = self.compute_log_prob(self.ref_brain, rejected_text)
            
            # DPO Loss
            # logits = self.beta * ((policy_chosen_logps - ref_chosen_logps) - (policy_rejected_logps - ref_rejected_logps))
            # loss = -torch.nn.functional.logsigmoid(logits)
            
            # loss.backward()
            # optimizer.step()
            # total_loss += loss.item()
            pass
            
        print("DPO Step complete (Placeholder).")

    def run_iteration(self):
        print("Starting Iterative DPO Cycle...")
        # 1. Generate & Rate
        samples = self.generate_and_rate(num_samples=5)
        
        # 2. Form Pairs
        pairs = self.form_pairs(samples)
        
        # 3. Train
        if pairs:
            self.dpo_step(pairs)
            
            # 4. Update Reference Model (slowly or periodically)
            # self.ref_brain = copy.deepcopy(self.interface.brain)
            pass
        else:
            print("Not enough pairs to train.")

if __name__ == "__main__":
    trainer = DPOTrainer()
    trainer.run_iteration()
